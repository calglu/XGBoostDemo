from sklearn.ensemble import RandomForestRegressor
from skopt import BayesSearchCV
from skopt.space import Integer, Real
import pandas as pd
import numpy as np
from sklearn.metrics import mean_squared_error


# Seed for reproducibility
seed = 42
np.random.seed(seed)

# Data generation
n = 1000
p = 50
data = pd.DataFrame(np.random.normal(size=(n, p)), columns=[f'X{i+1}' for i in range(p)])
data['y'] = (1
             + np.sin(data['X1']) * np.log1p(np.abs(data['X2']))
             + np.where(data['X5'] < 1, data['X3']**2 * data['X4'],0)
             + np.where(data['X3']>1, np.sqrt(np.abs(data['X5'])) * 5,0)
             + np.where(data['X6'] > 0, data['X6'], 0) ** 3) + np.random.normal(size=n)

# Splitting data into training and testing datasets
train_indices = np.random.choice(data.index, size=int(n * 0.8), replace=False)
DataTrain = data.loc[train_indices]
DataTest = data.drop(train_indices)


# Random Forest model setup
model = RandomForestRegressor(random_state=seed)

# Parameter space for Random Forest
rf_param_space = {
    'max_depth': Integer(1, 15),
    'min_samples_split': Integer(2, 10),
    'n_estimators': Integer(100, 1000)
}

# Bayesian optimization setup for Random Forest
rf_bayes_search = BayesSearchCV(estimator=model, search_spaces=rf_param_space, n_iter=32, cv=10, scoring='neg_mean_squared_error', verbose=2, n_jobs=-1, random_state=seed)
rf_bayes_search.fit(DataTrain.drop('y', axis=1), DataTrain['y'])

# Evaluate the best Random Forest model
rf_best_model = rf_bayes_search.best_estimator_
rf_predictions = rf_best_model.predict(DataTest.drop('y', axis=1))
rf_test_mse = mean_squared_error(DataTest['y'], rf_predictions)

print("Random Forest - Best parameters found: ", rf_bayes_search.best_params_)
print("Random Forest - Test MSE: ", rf_test_mse)
